#!/usr/bin/env python3
import hashlib
import os
import sys
from pathlib import Path
from posixpath import split

import numpy as np
import numpy.typing as npt
import pandas as pd
import pefile


# don't show tensorflow warnings
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

CHUNK_SIZE = 16384
DATA_DIR = Path("data")
# DATA_DIR = Path("/data/arpank/malwareproject/data")
MODEL_DIR = DATA_DIR / "saved_models" / "model1"
ID_TO_CLASS_MAP = {
    k: v
    for k, v in zip(
        range(9),
        (
            "Ramnit",
            "Lollipop",
            "Kelihos_ver3",
            "Vundo",
            "Simda",
            "Tracur",
            "Kelihos_ver1",
            "Obfuscator.ACY",
            "Gatak",
        ),
    )
}
THRESHOLD = 0.7
WHITELIST_FILE = DATA_DIR / "whitelist.txt"


def sum_rule_agg(y, chunk_lens_cum):
    return np.array(
        [res.mean(axis=0).argmax() for res in np.split(y, chunk_lens_cum[:-1])]
    )


def split_binary(X: npt.NDArray[np.uint8], chunk_size: int):
    """Split a numpy array into chunks of given size"""
    X_new = [X[start : start + chunk_size] for start in range(0, len(X), chunk_size)]
    # pad last chunk if it is smaller: or should we ignore it?
    if len(X_new[-1]) != chunk_size:
        X_new[-1] = np.pad(X_new[-1], (0, chunk_size - len(X_new[-1])))
    # y = to_categorical(np.repeat(np.uint(y), len(X_new)), NUM_CLASSES)
    return np.array(X_new), len(X_new)


def read_file_without_pe_header(fpath):
    try:
        pe = pefile.PE(fpath)
    except Exception as e:
        header = b""
    else:
        header = pe.header
    with fpath.open("br") as f:
        return np.frombuffer(f.read().replace(header, b""), dtype=np.uint8)


def predict(fpath: Path):
    from tensorflow.keras.models import load_model

    binary = read_file_without_pe_header(fpath)
    X, chks = split_binary(binary, CHUNK_SIZE)
    X = X[..., np.newaxis]
    model = load_model(MODEL_DIR)
    y_pred = model.predict(X)
    y_pred_agg = sum_rule_agg(y_pred, [chks])
    y_pred_agg = y_pred.mean(axis=0)
    pred_class = y_pred_agg.argmax()
    print(f"prediction probabilities: {y_pred_agg}")
    return pred_class, y_pred_agg[pred_class]


def get_sha256(fpath: Path):
    h = hashlib.sha256()

    with fpath.open("rb") as f:
        while True:
            # Reading is buffered, so we can read smaller chunks.
            chunk = f.read(h.block_size)
            if not chunk:
                break
            h.update(chunk)

    return h.hexdigest()


def check_whitelist(fpath: Path):
    # path,sha256,pred_class,prob
    if not WHITELIST_FILE.exists():
        return
    df = pd.read_csv(WHITELIST_FILE)
    df = df[df["path"] == str(fpath)]
    if len(df) > 0:
        entry = df.to_dict(orient="records")[0]
        sha256 = get_sha256(fpath)
        if sha256 == entry["sha256"]:
            return entry["pred_class"], entry["pred_prob"]


def add_to_whitelist(fpath: Path, pred_class, pred_prob):
    # path,sha256,pred_class,prob
    if WHITELIST_FILE.exists():
        wl = pd.read_csv(WHITELIST_FILE).to_dict(orient="records")
    else:
        wl = []
    wl.append(
        {
            "path": str(fpath),
            "sha256": get_sha256(fpath),
            "pred_class": pred_class,
            "pred_prob": pred_prob,
        }
    )
    df = pd.DataFrame(wl)
    df.to_csv(WHITELIST_FILE, index=False)


if __name__ == "__main__":
    fpath = Path(sys.argv[1])
    res = check_whitelist(fpath)
    if res is None:
        pred_class, pred_prob = predict(fpath)
        add_to_whitelist(fpath, pred_class, pred_prob)
    else:
        pred_class, pred_prob = res
    print(f"predicted class={ID_TO_CLASS_MAP[pred_class]} with probability={pred_prob}")
    if pred_prob > THRESHOLD:
        sys.exit(1)
    else:
        sys.exit(0)
